{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from injector import Module, Binder, singleton, provider\n",
    "from rag.manager.llm_manager import LLMManager\n",
    "from rag.manager.embed_manager import EmbeddingManager\n",
    "from rag.manager.vector_store_manager import VectorStoreManager\n",
    "from rag.manager.index_manager import IndexManager\n",
    "from rag.manager.node_manager import NodeManager\n",
    "from rag.config import Config\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "class ChatModule(Module):\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_config(self) -> Config:\n",
    "        return Config()  # Ensure this config is instantiated with the correct settings\n",
    "\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_llm_manager(self, config: Config) -> LLMManager:\n",
    "        return LLMManager(config)\n",
    "\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_embedding_manager(self, config: Config) -> EmbeddingManager:\n",
    "        return EmbeddingManager(config)\n",
    "\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_vector_store_manager(self, config: Config) -> VectorStoreManager:\n",
    "        return VectorStoreManager(config)\n",
    "\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_node_manager(self, config: Config) -> NodeManager:\n",
    "        return NodeManager(config)\n",
    "\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_storage_context(self, vector_store_manager: VectorStoreManager, node_manager: NodeManager) -> StorageContext:\n",
    "        return StorageContext.from_defaults(\n",
    "            vector_store=vector_store_manager.vector_store,\n",
    "            docstore=node_manager.doc_store,\n",
    "            index_store=node_manager.index_store,\n",
    "        )\n",
    "\n",
    "    @singleton\n",
    "    @provider\n",
    "    def provide_index_manager(self, config: Config, storage_context: StorageContext, embedding_manager: EmbeddingManager) -> IndexManager:\n",
    "        return IndexManager(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embedding_manager.embedding_model,\n",
    "            local_data_path=config.LOCAL_DATA_PATH,\n",
    "            show_progress=config.SHOW_PROGRESS,\n",
    "        )\n",
    "\n",
    "    def configure(self, binder: Binder) -> None:\n",
    "        # The bindings are now handled by the provider methods\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from injector import Injector\n",
    "from rag.services.chat_service import ChatService  \n",
    "\n",
    "injector = Injector([ChatModule()])\n",
    "\n",
    "# chat_service = injector.get(ChatService)\n",
    "# chat_service = injector.get(IndexManager)\n",
    "\n",
    "# message = \"What are the current interest rates according to the bank documents?\"\n",
    "# response = chat_service.chat(message)\n",
    "\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported LlamaIndex\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing c:\\Users\\shres\\Desktop\\DocParser.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shres\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 250.00it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:21<00:00, 10.52s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 527.85it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:11<00:00,  5.70s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 803.35it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.11s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 377.93it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:11<00:00,  5.89s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1410.80it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.89s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1144.11it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.70s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 414.83it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:11<00:00,  5.59s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1498.50it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.71s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 996.27it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.35s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 878.39it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.95s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1646.76it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 868.57it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:11<00:00,  5.85s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 866.41it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.13s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 860.37it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.28s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 1677.05it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:16<00:00,  8.28s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 736.23it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:19<00:00, 19.12s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 656.69it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 872.72it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:15<00:00, 15.26s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='6e2fff36-f878-4fa5-84e7-465af9027320', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 1, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='## DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents\\n\\nMohamed Dhouib[⋆][1][,][2[0000][−][0002][−][5587][−][1028]], Ghassen\\nBettaieb[1[0000][−][0003][−][3314][−][867][X][]], and Aymen Shabou[1[0000][−][0001][−][8933][−][7053]]\\n\\n\\n1 DataLab Groupe, Credit Agricole S.A, Montrouge, France\\n2 Ecole polytechnique, Palaiseau, France\\n```\\n       mohamed.dhouib@polytechnique.edu[∗]\\n\\n```\\n_{ghassen.bettaieb,aymen.shabou}@credit-agricole-sa.fr_\\n```\\n       https://datalab-groupe.github.io/\\n\\n```\\n**Abstract. Information Extraction from visually rich documents is a**\\nchallenging task that has gained a lot of attention in recent years due\\nto its importance in several document-control based applications and its\\nwidespread commercial value. The majority of the research work conducted on this topic to date follow a two-step pipeline. First, they read\\nthe text using an off-the-shelf Optical Character Recognition (OCR) engine, then, they extract the fields of interest from the obtained text. The\\nmain drawback of these approaches is their dependence on an external\\nOCR system, which can negatively impact both performance and computational speed. Recent OCR-free methods were proposed to address the\\nprevious issues. Inspired by their promising results, we propose in this\\npaper an OCR-free end-to-end information extraction model named DocParser. It differs from prior end-to-end approaches by its ability to better\\nextract discriminative character features. DocParser achieves state-ofthe-art results on various datasets, while still being faster than previous\\nworks.\\n\\n\\n**Keywords: Information Extraction · Visually Rich Documents · OCR-**\\nfree · End-to-end · DocParser\\n\\n### 1 Introduction\\n\\n\\nInformation extraction from visually rich documents (VRDs) is an important research topic that continues to be an active area of research [18,19,47,35,17,22,4,43,28]\\ndue to its importance in various real-world applications.\\nThe majority of the existing information extraction from visually rich documents approaches [17,10,36,14] depend on an external deep-learning-based Optical Character Recognition (OCR) [2,1] engine. They follow a two-step pipeline:\\nFirst they read the text using an off-the-shelf OCR system then they extract\\nthe fields of interest from the OCR’ed text. These two-step approaches have significant limitations due to their dependence on an external OCR engine. First\\n\\n_⋆_ The corresponding author\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68e118a0-dbd8-4ee0-9e72-8128414eb908', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 2, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='2 M. Dhouib et al.\\n\\nof all, these approaches need positional annotations along with textual annotations for training. Also, training an OCR model requires large scale datasets\\nand huge computational resources. Using an external pre-trained OCR model\\nis an option, which can degrade the whole model performance in the case of\\na domain shift. One way to tackle this is to fine-tune these off-the-shelf OCR\\nmodels which is still a delicate task. In fact, the documents full annotations are\\ngenerally needed to correctly fine-tune off-the-shelf OCR models, which is timeconsuming and difficult to obtain. OCR post-correction [38,21] is an option to\\ncorrect some of the recognition errors. However, this brings extra computational\\nand maintenance cost. Moreover, these two-step approaches rarely fully exploit\\nthe visual information because incorporating the textual information is already\\ncomputationally expensive.\\nRecent end-to-end OCR-free information extraction approaches [12,4,20] were\\nproposed to tackle some of the limitations of OCR-dependant approaches. The\\nmajority of these approaches follow an encoder-decoder scheme. However, the\\nused encoders are either unable to effectively model global dependence when\\nthey are primarily composed of Convolutional neural network (CNN) blocks\\n\\n[22,12] or they don’t give enough privilege to character-level features extraction\\nwhen they are are primarily composed of Swin Transformer [30] blocks [20,5].\\nIn this paper, we argue that capturing both intra-character local patterns and\\ninter-character long-range connections is essential for the information extraction\\ntask. The former is essential for character recognition and the latter plays a role\\nin both the recognition and the localization of the fields of interest.\\nMotivated by the issues mentioned above, we propose an end-to-end OCRfree information extraction model named DocParser. DocParser has been designed in a way that allows it to efficiently perceive both intra-character patterns\\nand inter-character dependencies. Consequently, DocParser is up to two times\\nfaster than state-of-the-art methods while still achieving state-of-the-art results\\non various datasets.\\n\\n### 2 Related Work\\n\\n**2.1** **OCR-dependant Approaches**\\n\\nMost of the OCR-dependant approaches simply use an off-the-shelf OCR engine\\nand only focus on the information extraction task.\\nPrior to the development of deep learning techniques, earlier approaches\\n\\n[37,34,3] either followed a probabilistic approach, relied on rules or used manually\\ndesigned features which often results in failure when applied to unfamiliar templates. The initial deep learning approaches only relied on textual information\\nand simply used pre-trained language models [7,29]. Later, several approaches\\ntried to take the layout information into consideration. First, [18] proposed Chargrid, a new type of text representation that preserves the 2D layout of a document by encoding each document page as a two-dimensional grid of characters.\\nThen, [6] added context to this representation by using a BERT language model.\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9baf357-c90c-4987-9e2f-2ba203f8718d', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 3, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 3\\n\\nLater, [19] improved the Chargrid model by also exploiting the visual information. Graph-based models were also proposed to exploit both textual and visual\\ninformation [28,39].\\n\\nTo successfully model the interaction between the visual, textual and positional information, recent approaches [17,10,36,14] resorted to pre-training large\\nmodels. First [46] tried to bring the success of large pre-trained language models\\ninto the multi-modal domain of document understanding and proposed LayoutLM. LayoutLMv2 [45] was later released where new pre-training tasks were\\nintroduced to better capture the cross-modality interaction in the pre-training\\nstage. The architecture was also improved by introducing spatially biased attention and thus making the spatial information more influential. Inspired by the\\nVision Transformer (ViT) [23], [17] modified LayoutLMv2 by using patch embeddings instead of a ResNeXt [44] Feature Pyramid Network [27] visual backbone\\nand released LayoutLMv3. Pre-training tasks were also improved compared to\\nprevious versions. [10] proposed LAMBERT which used a modified RoBERTa\\n\\n[29] that also exploits the layout features obtained from an OCR system. [36]\\nproposed TILT, a pre-trained encoder-decoder model. [14] tried to fully exploit\\nthe textual and layout information and released Bros which achieves good results\\nwithout relying on the visual features. However, the efficiency and the computational cost of all the previously cited works are still hugely impacted by the\\nused OCR system.\\n\\n**2.2** **End-to-end Approaches**\\n\\nIn recent years, end-to-end approaches were proposed for the information extraction task among many other Visually-Rich Document Understanding (VRDU)\\ntasks. [12,22] both used a CNN-based encoder and a recurrent neuronal network\\ncoupled with an attention mechanism decoder. However, the accuracy of these\\ntwo approaches is limited and they perform relatively badly on small datasets. [4]\\nproposed TRIE++, a model that learns simultaneously both the text reading and\\nthe information extraction tasks via a multi-modal context block that bridges\\nthe visual and natural language processing tasks. [41] released VIES which simultaneously performs text detection, recognition and information extraction.\\nHowever, both TRIE++ and VIES require the full document annotation to be\\ntrained. [20] proposed Donut, an encoder-decoder architecture that consists of a\\nSwin Transformer [30] encoder and a Bart [25]-like decoder. [5] released Dessurt,\\na model that processes three streams of tokens, representing visual tokens, query\\ntokens and the response. Cross-attention is applied across different streams to\\nallow them to share and transfer information into the response. To process the\\nvisual tokens, Dessurt uses a modified Swin windowed attention that is allowed\\nto attend to the query tokens. Donut and Dessurt achieved promising results,\\nhowever, they don’t give enough privilege to local character patterns which leads\\nto sub-optimal results for the information extraction task.\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebf2315e-8866-42a2-9ffa-2e1abfab8c77', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 4, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='4 M. Dhouib et al.\\n\\n_H × W × 3_\\n\\n\\n_<company>_ CROSS CHANNEL NETWORK\\n\\n**Decoder**\\n\\nHead\\n\\n_H_\\n32 _[×][ W]32_ _[×][ C][5]_\\n\\n**Encoder** Feature map cross-attention\\n\\nself-attention\\n\\n\\n_<sroie>_ _<company>_ CROSS CHANNEL\\n\\n\\n**Fig. 1. An overview of DocParser’s architecture. The input image of size H ×**\\n_W × 3 is first encoded to generate a feature map of size_ 32H _[×][ W]32_ _[×][ C][5][ containing]_\\n\\nrelevant visual information. The feature map is then fed to the decoder along with a\\ntask token to auto-regressively generate tokens that represent the fields of interest. For\\nthe purpose of simplification, the figure does not include residual connections and the\\nfeed-forward sub-layer.\\n\\n### 3 Proposed Method\\n\\nThis section introduces DocParser, our proposed end-to-end information extraction from VRDs model.\\nGiven a document image and a task token that determines the fields of interest, DocParser produces a series of tokens representing the extracted fields\\nfrom the input image. DocParser architecture consists of a visual encoder followed by a textual decoder. An overview of DocParser’s architecture is shown\\non figure 1. The encoder consists of a three-stage progressively decreased height\\nconvolutional neural network that aims to extract intra-character local patterns,\\nfollowed by a three-stage progressively decreased width Swin Transformer [30]\\nthat aims to capture long-range dependencies. The decoder consists of n Transformer layers. Each layer is principally composed of a multi-head self-attention\\nsub-layer followed by a multi-head cross-attention sub-layer and a feed-forward\\nsub-layer as explained in [40].\\n\\n**3.1** **Encoder**\\n\\nThe encoder is composed of six stages. The input of the encoder is an image\\nof size H × W × 3. It is first transformed to _[H]4_ _[×][ W]4_ [patches of dimension][ C][0]\\n\\nvia an initial patch embedding. Each patch either represents a fraction of a\\ntext character or a fraction of a non-text component of the input image. First,\\nthree stages composed of ConvNext [31] blocks are applied at different scales\\nfor character-level discriminative features extraction. Then three stages of Swin\\nTransformer blocks are applied with varying window sizes in order to capture\\nlong-range dependencies. The output of the encoder is a feature map of size\\n_H_\\n32 _[×][ W]32_ _[×][ C][5][ that contains multi-grained features. An overview of the encoder]_\\n\\narchitecture is illustrated in figure 2.\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73e7fde8-540a-4c2d-aa62-b469ab8af7a7', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 5, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 5\\n\\n\\n_H_ _H_ _H_ _H_ _H_ _H_ _H_\\n\\n4 _[. W]4_ _[.C][0]_ 8 _[. W]4_ _[.C][1]_ 16 _[. W]4_ _[.C][2]_ 32 _[. W]8_ _[.C][3]_ 32 _[. W]16_ _[.C][4]_ 32 _[. W]32_ _[.C][5]_ 32 _[. W]32_ _[.C][5]_\\n\\nStage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6\\n\\n.W .3\\n\\nCN CN CN Swin Swin Swin\\n\\n(7, 7) (7, 7) (7, 7) (5, 40) (5, 20) (10, 10)\\n\\n_×3_ _×6_ _×6_ _×2_ _×2_ _×2_\\n\\n\\n**Fig. 2. The architecture of DocParser’s encoder. H and W represent the height**\\nand the width of the input image. Ci, i ∈ [0..5] represent the number of channels at\\ndifferent stages. The first three stages are composed of ConvNext (CN) blocks with\\na filter size of (7, 7). The last three stages are composed of Swin Transformer blocks\\nwith different attention window sizes. The windows’ heights and widths are respectively\\nequal to (5,40), (5,20) and (10,10).\\n\\n**Patch Embedding**\\n\\n\\nSimilar to [8], we use a progressive overlapping patch embedding. For an input image of size W _H_ 3, a 3 3 convolution with stride 2 is first applied\\n_×_ _×_ _×_\\nto have an output of size _[W]2_ 2 2 [. It is then followed by a normalization]\\n\\n_[×][ H]_ _[×][ C][0]_\\n\\nlayer and another 3 3 convolution with stride 2. The size of the final output is\\n_×_\\n_W_\\n\\n4 _[×][ H]4_ _[×][ C][0][.]_\\n\\n**ConvNext-based Stages**\\n\\nThe first three stages of DocParser’s encoder are composed of ConvNext blocks.\\nEach stage is composed of several blocks. The kernel size is set to 7 for all stages.\\nAt the end of each stage, the height of the feature map is reduced by a factor\\nof two and the number of channels Ci, i ∈ [1, 2, 3] is increased to compensate\\nfor the information loss. The feature map width is also reduced by a factor of\\ntwo at the end of the third stage. The role of these blocks is to capture the correlation between the different parts of each single character and to encode the\\nnon-textual parts of the image. We don’t reduce the width of the feature map between these blocks in order to avoid encoding components of different characters\\nin the same feature vector and thus allowing discriminative character features\\ncomputation. We note that contrary to the first encoder stages where low-level\\nfeatures extraction occurs, encoding components of different characters in the\\nsame feature vector doesn’t affect performance if done in the encoder last stages\\nwhere high-level features are constructed. This is empirically demonstrated in\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ca5b6fc3-18f1-4587-9020-a5159fedb7d2', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 6, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='6 M. Dhouib et al.\\n\\nsection 5. We chose to use convolutional blocks for the early stages mainly due\\nto their good ability at modeling local correlation at a low computational cost.\\n\\n**Swin Transformer-based Stages**\\n\\nThe last three stages of the encoder are composed of Swin Transformer blocks.\\nWe modify Swin’s window-based multi-head self-attention to be able to use rectangular attention windows. At the output of the fourth and fifth stages, the width\\nof the feature map is reduced by a factor of two and the number of channels is\\nincreased to compensate for the information loss. The role of these layers is to\\ncapture the correlation between the different characters of the input image or\\nbetween textual and non-textual components of the image. In the forth and fifth\\nstage, the encoder focuses on capturing the correlation between characters that\\nbelong to adjacent sentences. This is accomplished through the use of horizontally wide windows, as text in documents typically has an horizontal orientation.\\nIn the last stage, the encoder focuses on capturing long-range context in both\\ndirections. This is achieved through the use of square attention windows. As\\na result, the output of the encoder is composed of multi-grained features that\\nnot only encode intra-character local patterns which are essential to distinguish\\ncharacters but also capture the correlation between textual and non-textual components which is necessary to correctly locate the fields of interest. We note that\\npositional embedding is added to the encoder’s feature map before the encoder’s\\nforth stage.\\n\\n**3.2** **Decoder**\\n\\nThe decoder takes as input the encoder’s output and a task token. It then outputs\\nautoregressively several tokens that represent the fields of interest specified by\\nthe input token. The decoder consists of n[1] layers, each one is similar to a vanilla\\nTransformer decoder layer. It consists of a multi-head self-attention sub-layer\\nfollowed by a multi-head cross-attention sub-layer and a feed-forward sub-layer.\\n\\n**Tokenization We use the tokenizer of the RoBERTa model [29] to transform**\\nthe ground-truth text into tokens. This allows to reduce the number of generated\\ntokens, and so the memory consumption as well as training and inference times,\\nwhile not affecting the model performance as shown in section 5. Similar to [20],\\nspecial tokens are added to mark the start and the end of each field or group\\nof fields. Two additional special tokens < item > and < item/ > are used to\\nseparate fields or group of fields appearing more than once in the ground truth.\\nAn example is shown in figure 3.\\n\\n**At Training Time When training the model, we use a teacher forcing strategy.**\\nThis means that we give the decoder all the ground truth tokens as input. Each\\n\\n1 For our final model, we set n=1\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d37c4e0b-95dd-403b-8e08-f9ac4ab6f8e0', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 7, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 7\\n\\n_{_\\n”menu”: [\\n_{”name”: ”Macchiato”, ”price”: ”17500”},_\\n_{”name”: ”TEA”, ”price”: ”5000”},_\\n],\\n”total”: {”total-price”: ”22500”, ”cash-price”: ”22500”}\\n_}_\\n\\n_<Task >_ _<menu >_ _<item >_ _<name >_ Mac chi ato _<name />_\\n\\n_<price >_ 17 500 _<price />_ _<item />_ _<item >_ _<name >_ TEA\\n\\n_<name />_ _<price >_ 5000 _<price />_ _<item />_ _<menu />_ _<total >_\\n\\n_<total-price >_ 22 500 _<total-price />_ _<cash-price >_ 22 500\\n\\n_<cash-price />_ _<total />_ _<End >_\\n\\n\\n**Fig. 3. An illustration of the processing applied to the textual ground-truth.**\\n\\ninput token corresponding last hidden state is used to predict the next token.\\nTo ensure that each token only attends to previous tokens in the self-attention\\nlayer, we use a triangular attention mask that masks the following tokens.\\n\\n### 4 Expriments and Results\\n\\n**4.1** **Pre-training**\\n\\nWe pre-train our model on two different steps :\\n\\n**Knowledge Transfer Step Using an L2 Loss, we teach the ConvNext-based**\\nencoder blocks to produce the same feature map as the PP-OCR-V2 [9] recognition backbone which is an enhanced version of MobileNetV1 [15]. A pointwise\\nconvolution is applied to the output of the ConvNext-based blocks in order to\\nobtain the same number of channels as the output of PP-OCR-V2 recognition\\nbackbone. The goal of this step is to give the encoder the ability to extract\\ndiscriminative intra-character features. We use 0.2 million documents from the\\nIIT-CDIP [24] dataset for this task. We note that even though PP-OCR-V2\\nrecognition network was trained on text crops, the features generated by its\\nbackbone on a full image are still useful thanks to the translation equivariance\\nof CNNs.\\n\\n**Masked Document Reading Step After the knowledge transfer step, we**\\npre-train our model on the task of document reading. In this pre-training phase,\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6957bf59-435e-4657-b33a-0d78bc2b92b0', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 8, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='8 M. Dhouib et al.\\n\\nthe model learns to predict the next textual token while conditioning on the\\nprevious textual tokens and the input image. To encourage joint reasoning, we\\nmask several 32 32 blocks representing approximately fifteen percent of the\\n_×_\\ninput image. In fact, in order to predict the text situated within the masked\\nregions, the model is obliged to understand its textual context. As a result,\\nDocParser learns simultaneously to recognize characters and the underlying language knowledge. We use 1.5 million IIT-CDIP documents for this task. These\\ndocuments were annotated using Donut. Regex rules were applied to identify\\npoorly read documents, which were discarded.\\n\\n**4.2** **Fine-tuning**\\n\\nAfter the pre-training stage, the model is fine-tuned on the information extraction task. We fine-tune DocParser on three datasets: SROIE and CORD which\\nare public datasets and an in-house private Information Statement Dataset.\\n\\n_SROIE : A public receipts dataset with 4 annotated unique fields : company,_\\ndate, address, and total. It contains 626 receipts for training and 347 receipts\\nfor testing.\\n\\n_CORD : A public receipts dataset with 30 annotated unique fields of interest._\\nIt consists of 800 train, 100 validation and 100 test receipt images.\\n\\n_Information Statement Dataset (ISD) : A private information statement dataset_\\nwith 18 annotated unique fields of interest. It consists of 7500 train, 3250 test\\nand 3250 eval images. The documents come from 15 different insurers, each insurer has around 4 different templates. We note that for the same template, the\\nstructure can vary depending on the available information. On figure 4 we show\\n3 samples from 3 different insurers.\\n\\n**4.3** **Evaluation Metrics**\\n\\nWe evaluate our model using two metrics:\\n\\n**Field-level F1 Score The field-level F1 score checks whether each extracted**\\nfield corresponds exactly to its value in the ground truth. For a given field,\\nthe field-level F1 score assumes that the extraction has failed even if one single\\ncharacter is poorly predicted. The field-level F1 score is described using the\\nfield-level precision and recall as:\\n\\nPrecision = [The number of exact field matches] (1)\\n\\nThe number of the detected fields\\n\\nThe number of exact field matches\\nRecall = (2)\\n\\nThe number of the ground truth fields\\n\\n\\nF1 = [2][ ×][ Precision][ ×][ Recall] (3)\\n\\nPrecision + Recall\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b38586e3-a770-4e08-8436-753a3ddfb34d', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 9, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 9\\n\\n**Fig. 4. Anonymized samples from our private in-house dataset. The fields of**\\ninterest are located within the red boxes.\\n\\n**Document Accuracy Rate (DAR) This metric evaluates the number of**\\ndocuments that are completely and correctly processed by the model. If for\\na given document we have even one false positive or false negative, the DAR\\nassumes that the extraction has failed. This metric is a challenging one, but\\nrequested in various industrial applications where we need to evaluate at which\\nextent the process is fully automatable.\\n\\n**4.4** **Setups**\\n\\nThe dimension of the input patches and the output vectors of every stage Ci,\\n_i_ [0 . . 5] are respectively set to 64, 128, 256, 512, 768, and 1024. We set\\n_∈_\\nthe number of decoder layers to 1. This choice is explained in Section 5. For\\nboth pre-training and fine-tuning we use the Cross-Entropy Loss, AdamW [33]\\noptimizer with weight decay of 0.01 and stochastic depth [16] with a probability\\nequal to 0.1. We also follow a light data augmentation strategy which consists\\nof light re-scaling and rotation as well as brightness, saturation, and contrast\\naugmentation applied to the input image. For the pre-training phase, we set the\\ninput image size to 2560 1920. The learning rate is set to 1e 4. The pre_×_ _−_\\ntraining is done on 7 A100 GPUs with a batch size of 4 on each GPU. We use\\ngradient accumulation of 10 iterations, leading to an effective batch size of 280.\\nFor the fine-tuning, the resolution is set to 1600 960 for CORD and SROIE\\n_×_\\ndatasets and 1600 1280 for the Information Statement Dataset. We pad the\\n_×_\\ninput image in order to maintain its aspect ratio. We also use a Cosine Annealing\\nscheduler [32] with an initial learning rate of 3e 5 and a batch size of 8.\\n_−_\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ad5b5e4-0b13-43c0-8ca1-ce64bc528c14', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 10, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='10 M. Dhouib et al.\\n\\n**Table 1. Performance comparisons on the three datasets. The field-level F1-**\\nscore and the extraction time per image on an Intel Xenion W-2235 CPU are reported.\\nIn order to ensure a fair comparison, we exclude parameters related to vocabulary.\\nAdditional parameters α[∗] and time t[∗] for the OCR step should be considered for\\nLayouLM-v3. For the ISD dataset t[∗] is equal to 3.6 seconds.\\n\\nSROIE CORD ISD\\n\\nOCR Params(M) F1(%) Time(s) F1(%) Time(s) F1(%) Time(s)\\n\\nLayoutLM-v3 � 87 + α[∗] 77.7 2.1 + t[∗] 80.2 2.1 + t[∗] 90.8 4.1 + t[∗]\\n\\nDonut 149 81.7 5.3 84 5.7 95.4 6.7\\nDessurt 87 84.9 16.7 82.5 17.9 93.5 18.1\\n**DocParser** **70** **87.3** 3.5 **84.5** 3.7 **96.2** **4.4**\\n\\n**4.5** **Results**\\n\\nWe compare DocParser to Donut, Dessurt and LayoutLM-v3. The results are\\nsummarized in table 1. A comparison of inference speed on an NVIDIA Quadro\\nRTX 6000 GPU is presented in table 2. Per-field extraction performances on our\\nInformation Statement Dataset can be found in table 3. DocParser achieves a\\nnew state-of-the-art on SROIE, CORD and our Information Statement Dataset\\nwith an improvement of respectively 2.4, 0.5 and 0.8 points over the previous\\nstate-of-the-art. In addition, Docparser has a significantly faster inference speed\\nand less parameters.\\n\\n**Table 2. Comparison of inference speed on GPU. Extraction time (seconds) per**\\nimage on an NVIDIA Quadro RTX 6000 GPU is reported. Additional time t[∗] for the\\nOCR step should be considered for LayouLM-v3. For the ISD dataset t[∗] is equal to 0.5\\nseconds.\\n\\nSROIE CORD ISD\\n\\nLayoutLM-v3 0.041 + t[∗] 0.039 + t[∗] 0.065 + t[∗]\\n\\nDonut 0.38 0.44 0.5\\nDessurt 1.2 1.37 1.39\\n**DocParser** 0.21 0.24 **0.25**\\n\\nRegarding the OCR required by the LayoutLM-v3 approach, we use, for\\nboth SROIE and CORD datasets, Microsoft Form Recognizer[2] which includes\\na document-optimized version of Microsoft Read OCR (MS OCR) as its OCR\\nengine. We note that we tried combining a ResNet-50 [13]-based DBNet++ [26]\\nfor text detection and an SVTR [8] model for text recognition and fine-tuned\\n\\n2 https://learn.microsoft.com/en-us/azure/applied-ai-services/formrecognizer/concept-read?view=form-recog-3.0.0\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f30c16f6-c381-41da-b562-05d9b91c490d', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 11, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 11\\n\\n**Table 3. Extraction performances on our Information Statement Dataset.**\\nPer field (field-level) F1-score, field-level F1-score mean, DAR, and extraction time per\\nimage on an Intel Xenion W-2235 CPU are reported. The OCR engine inference time\\n_t[∗]_ should be considered for LayouLM-v3.\\n\\n|Fields|LayoutLM DONUT Dessurt DocParser|\\n|---|---|\\n|Name of first driver Name of second driver Name of third driver Bonus-Malus Date of birth of first driver Date of birth of second driver Date of birth of third driver Date of termination of contract Date of the first accident Date of the second accident Subscription date Date of creation of the document Matriculation Name of first accident’s driver Name of second accident’s driver Underwriter Responsibility of the first driver of the accident Responsibility of the second driver of the accident|85.7 91.3 89.7 92.9 82.7 90.1 89.1 92.1 76.8 94.2 91.7 94.7 96 98.5 98.1 99 97.1 97.1 97.3 98.3 95.8 96.9 96.3 97.2 91.5 93.2 90.8 92.8 92 97 95.7 97.8 95.6 96 96.3 97.2 94.2 95.2 94.8 96.1 94.7 97.9 95.7 98.5 95.6 97.5 96.8 98 95.2 94 94.6 96.7 85.9 85.9 85.4 86.1 84.1 87.1 84.9 85.2 92.2 92.2 92.3 92.8 89.8 96.5 95.7 97.2 89.5 95 94 95.3|\\n|Mean F1 (%) DAR (%)|90.8 95.4 93.5 96.2 58.1 74 70.6 77.5|\\n|Time (s)|4.1+t∗ 6.7 18.1 4.4|\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d32f51f-b47a-4cae-a70b-ffdee8ed568a', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 12, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='12 M. Dhouib et al.\\n\\nthem on the fields of interest of each dataset. However, the obtained results are\\nworse than those obtained with Microsoft Form Recognizer OCR engine. For\\nthe Information Statement Dataset, we don’t use MS OCR for confidentiality\\npurposes. Instead, we use an in-house OCR fine-tuned on this dataset to reach\\nthe best possible performances. Even though the best OCRs are used for each\\ntask, LayoutLM-v3 extraction performances are still lower than those of OCRfree models. This proves the superiority of end-to-end architectures over the\\nOCR-dependent approaches for the information extraction task. We note that\\nfor Donut, we use the same input resolution as DocParser. For Dessurt, we use\\na resolution of 1152 768, which is the resolution used to pre-train the model.\\n_×_\\n\\n### 5 Primary Experiments and Further Investigation\\n\\n**5.1** **Primary Experiments**\\n\\nIn all the experiments, the tested architectures were pre-trained on 0.5 Million synthesized documents and fine-tuned on a deskewed version of the SROIE\\ndataset. We report the inference time on a an Intel Xenion W-2235 CPU, as we\\naim to provide a model suited for low resources scenarios.\\n\\n**Table 4. Comparison of different encoder architectures. The dataset used is a**\\ndeskewed version of the SROIE dataset. The field-level F1 score is reported.\\n\\nEncoder architecture F1(%)\\n\\nEasyOCR-based encoder 54\\n\\nPP-OCRv2-based encoder 77\\n\\nProposed encoder 81\\n\\n**On the Encoder’s Architecture The table 4 shows a comparison between**\\nan EasyOCR[3]-based encoder, a PP-OCRv2 [9]-based encoder and our proposed\\nDocParser encoder. Concerning the EasyOCR and PP-OCRv2 based encoders,\\neach one consists of its corresponding OCR’s recognition network followed by\\nfew convolutional layers that aim to further reduce the feature map size and\\nincrease the receptive field. Our proposed encoder surpasses both encoders by a\\nlarge margin.\\n\\n**On the Feature Map Width Reduction While encoding the input image,**\\nthe majority of the text recognition approaches reduce the dimensions of the\\nfeature map mainly vertically [8] [1]. Intuitively, applying this approach for the\\n\\n3 https://github.com/JaidedAI/EasyOCR/blob/master/easyocr\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b71dbee-65bb-42f5-87ce-ef88cb9ce8ae', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 13, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 13\\n\\n**Table 5. The effect of decreasing the width of the feature map in various**\\n**stages of DocParser’s encoder. The dataset used is a deskewed version of the**\\nSROIE dataset. The field-level F1-score and the extraction time per image on an Intel\\nXenion W-2235 CPU are reported.\\n\\nEncoder stages\\nInference time\\nwhere the feature Decoder F1(%)\\n(seconds)\\nmap width is reduced\\n\\n(3,4,5) (proposed) Transformer 3.5 81\\n\\n(3,4,5) LSTM + Additive attention 3.3 58\\n\\n(1,2,3) Transformer 2.1 69\\n\\n(1,2,3) LSTM + Additive attention 1.9 52\\n\\nNo reduction Transformer 6.9 81\\n\\nNo reduction LSTM + Additive attention 6.6 81\\n\\ninformation extraction task may seem relevant as it allows different characters\\nto be encoded in different feature vectors. Our empirical results, however, show\\nthat this may not always be the case. In fact, we experimented with reducing the\\nencoder’s feature map width at different stages. As a decoder, we used both a one\\nlayer vanilla Transformer decoder and a Long Short-Term Memory (LSTM) [11]\\ncoupled with an attention mechanism that uses an additive attention scoring\\nfunction [42]. Table 5 shows that reducing the width of the feature map in\\nthe early stages affects drastically the model’s accuracy and that reducing the\\nwidth of the feature map in the later stages achieves the the best speed-accuracy\\ntrade-off. Table 5 also shows that while the LSTM-based decoder struggles with a\\nreduced width encoder output, the performance of the vanilla Transformer-based\\ndecoder remains the same in both cases. This is probably due to the multi-head\\nattention mechanism that makes the Transformer-based decoder more expressive\\nthan an LSTM coupled with an attention mechanism.\\n\\n**On the Tokenizer Choice In addition to the RoBERTa tokenizer, we also**\\ntested a character-level tokenizer. Table 6 shows that the RoBERTa tokenizer\\nallows faster decoding while achieving the same performance as the characterlevel tokenizer.\\n\\n**On the Number of Decoder Layers Table 7 shows that increasing the**\\nnumber of decoder layers doesn’t improve DocParser’s performance. Therefore,\\nusing one decoder layer is the best choice as it guarantees less computational\\ncost.\\n\\n**On the Data Augmentation Strategy Additionally to the adopted augmen-**\\ntation techniques, we experimented with adding different types of blur and noise\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0db3954-7768-4524-bb93-46888da8c46c', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 14, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='14 M. Dhouib et al.\\n\\n**Table 6. Comparison between different tokenization techniques. The dataset**\\nused is a deskewed version of the SROIE dataset. The field-level F1-score and the\\ndecoding time per image on an Intel Xenion W-2235 CPU are reported.\\n\\nTokenizer Decoding inference time (s) F1(%)\\n\\nRoBERTa tokenizer 0.6 81\\n\\nCharacter-level tokenizer 1.2 81\\n\\nto the input images for both the pre-training and the fine-tuning. We concluded\\nthat this does not improve DocParser’s performance. The lack of performance\\nimprovement when using blur may be attributed to the fact that the datasets\\nused for evaluating the model do not typically include blurred images. Additionally, it is challenging to accurately create realistic noise, thus making the\\ntechnique of adding noise to the input images ineffective.\\n\\n**Table 7. Effect of the number of decoder layers on the performance and the**\\n**decoding inference time of DocParser. The dataset used is a deskewed version**\\nof the SROIE dataset. The field-level F1-score and the decoding time per image on an\\nIntel Xenion W-2235 CPU are reported.\\n\\nDecoder layers Decoding inference time (s) F1(%)\\n\\n1 0.6 81\\n\\n2 0.8 81\\n\\n4 1.2 81\\n\\n**5.2** **Further Investigation**\\n\\n**Table 8. Comparison between different pre-training strategies. All the models**\\nare pre-trained for a total of 70k steps. The field-level F1-score is reported.\\n\\nPre-training tasks SROIE CORD ISD\\n\\nKnowledge transfer 77.5 75 89.7\\n\\nKnowledge transfer + Document reading 84.7 83.7 95.6\\n\\nKnowledge transfer + Masked document reading **84.9** **84.2** **95.9**\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f72f643a-909b-4b50-b384-cf9a5d73b0b5', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 15, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 15\\n\\n**On the Pre-training Strategy Table 8 presents a comparison between dif-**\\nferent pre-training strategies. To reduce compute used, all the models were pretrained for 70k back-propagation steps, with 7k knowledge transfer steps in the\\ncase of two pre-training tasks. The results show that masking text regions during the document reading pre-training task does effectively lead to an increase\\nin performance on all three datasets. It also confirms, as demonstrated in [20]\\nand [5], that document reading, despite its simplicity, is an effective pre-training\\ntask.\\n\\n**On the Input Resolution Figure 5 shows the effect of the input resolution on**\\nthe performance of DocParser on the SROIE dataset. DocParser shows satisfying\\nresults even with a low-resolution input. It achieves 83.1 field-level F1 score with\\na 960 640 input resolution. The inference time for this resolution on an Intel\\n_×_\\nXenion W-2235 CPU is only 1.7 seconds. So, even at this resolution, DocParser\\nstill surpasses Donut and LayoutLM-v3 on SROIE while being more than three\\ntimes faster. However, if the input resolution is set to 640 640 or below, the\\n_×_\\nmodel’s performance shows a drastic drop. This may be due to the fact that the\\ncharacters start to be illegible at such a low resolution.\\n\\n**Fig. 5. The impact of the input resolution on DocParser’s performance on**\\n**the SROIE dataset. The field-level F1 score is reported.**\\n\\n### 6 Conclusion\\n\\nWe have introduced DocParser, a fast end-to-end approach for information extraction from visually rich documents. Contrary to previously proposed end\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7cfa064f-242e-49f0-86ce-da01227f5fa7', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 16, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='16 M. Dhouib et al.\\n\\nto-end models, DocParser’s encoder is specifically designed to capture both\\nintra-character local patterns and inter-character long-range dependencies. Experiments on both public and private datasets showed that DocParser achieves\\nstate-of-the-art results in terms of both speed and accuracy which makes it perfectly suitable for real-world applications.\\n\\n**Acknowledgments The authors wish to convey their genuine appreciation to**\\nProf. Davide Buscaldi and Prof. Sonia Vanier for providing them with valuable\\nguidance. Furthermore, the authors would like to express their gratitude to Paul\\nWassermann and Arnaud Paran for their assistance in proofreading previous\\nversions of the manuscript.\\n\\n### References\\n\\n1. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is\\nwrong with scene text recognition model comparisons? dataset and model analysis.\\nIn: Proceedings of the IEEE/CVF international conference on computer vision. pp.\\n4715–4723 (2019)\\n\\n2. Baek, Y., Lee, B., Han, D., Yun, S., Lee, H.: Character region awareness for text\\ndetection. In: Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition. pp. 9365–9374 (2019)\\n\\n3. Cesarini, F., Francesconi, E., Gori, M., Soda, G.: Analysis and understanding of\\nmulti-class invoices. Document Analysis and Recognition 6, 102–114 (2003)\\n\\n4. Cheng, Z., Zhang, P., Li, C., Liang, Q., Xu, Y., Li, P., Pu, S., Niu, Y., Wu, F.:\\nTrie++: Towards end-to-end information extraction from visually rich documents.\\narXiv preprint arXiv:2207.06744 (2022)\\n\\n5. Davis, B., Morse, B., Price, B., Tensmeyer, C., Wigington, C., Morariu, V.: End-toend document recognition and understanding with dessurt. In: Computer Vision–\\nECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\\nIV. pp. 280–296. Springer (2023)\\n\\n6. Denk, T.I., Reisswig, C.: Bertgrid: Contextualized embedding for 2d document representation and understanding. In Workshop on Document Intelligence at NeurIPS.\\n(2019)\\n\\n7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In: Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\\npp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota\\n(Jun 2019)\\n\\n8. Du, Y., Chen, Z., Jia, C., Yin, X., Zheng, T., Li, C., Du, Y., Jiang, Y.G.: Svtr: Scene\\ntext recognition with a single visual model. In: Raedt, L.D. (ed.) Proceedings of the\\nThirty-FirstInternational Joint Conference on Artificial Intelligence, IJCAI-22. pp.\\n884–890. International Joint Conferences on Artificial Intelligence Organization (7\\n2022), main Track\\n\\n9. Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu,\\nX., Yu, D., Ma, Y.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. ArXiv\\n**abs/2109.03144 (2021)**\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7734dde7-ae01-43ee-b3c6-0ac4d915fd0d', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 17, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 17\\n\\n10. Garncarek, �L., Powalski, R., Stanis�lawek, T., Topolski, B., Halama, P., Turski,\\nM., Grali´n ski, F.: LAMBERT: Layout-aware language modeling for information\\nextraction. In: Document Analysis and Recognition ICDAR 2021, pp. 532–547.\\nSpringer International Publishing (2021)\\n\\n11. Graves, A., Graves, A.: Long short-term memory. Supervised sequence labelling\\nwith recurrent neural networks pp. 37–45 (2012)\\n\\n12. Guo, H., Qin, X., Liu, J., Han, J., Liu, J., Ding, E.: Eaten: Entity-aware attention for single shot visual text extraction. In: 2019 International Conference on\\nDocument Analysis and Recognition (ICDAR). pp. 254–259 (2019)\\n\\n13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\\nProceedings of the IEEE conference on computer vision and pattern recognition.\\npp. 770–778 (2016)\\n\\n14. Hong, T., Kim, D., Ji, M., Hwang, W., Nam, D., Park, S.: Bros: A pre-trained\\nlanguage model focusing on text and layout for better key information extraction\\nfrom documents. In: Proceedings of the AAAI Conference on Artificial Intelligence.\\nvol. 36, pp. 10767–10775 (2022)\\n\\n15. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for\\nmobile vision applications (2017)\\n\\n16. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with\\nstochastic depth. In: Computer Vision–ECCV 2016: 14th European Conference,\\nAmsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14. pp.\\n646–661. Springer (2016)\\n\\n17. Huang, Y., Lv, T., Cui, L., Lu, Y., Wei, F.: Layoutlmv3: Pre-training for document ai with unified text and image masking. In: Proceedings of the 30th ACM\\nInternational Conference on Multimedia. p. 4083–4091. MM ’22, Association for\\nComputing Machinery, New York, NY, USA (2022)\\n\\n18. Katti, A.R., Reisswig, C., Guder, C., Brarda, S., Bickel, S., H¨ohne, J., Faddoul,\\nJ.B.: Chargrid: Towards understanding 2D documents. In: Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing. pp. 4459–4469.\\nAssociation for Computational Linguistics, Brussels, Belgium (Oct-Nov 2018)\\n\\n19. Kerroumi, M., Sayem, O., Shabou, A.: Visualwordgrid: Information extraction\\nfrom scanned documents using a multimodal approach. In: Document Analysis\\nand Recognition–ICDAR 2021 Workshops: Lausanne, Switzerland, September 5–\\n10, 2021, Proceedings, Part II. pp. 389–402. Springer (2021)\\n\\n20. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S.,\\nHan, D., Park, S.: Ocr-free document understanding transformer. In: Avidan, S.,\\nBrostow, G., Ciss´e, M., Farinella, G.M., Hassner, T. (eds.) Computer Vision –\\nECCV 2022. pp. 498–517. Springer Nature Switzerland, Cham (2022)\\n\\n21. Kissos, I., Dershowitz, N.: Ocr error correction using character correction and\\nfeature-based word classification. In: 2016 12th IAPR Workshop on Document\\nAnalysis Systems (DAS). pp. 198–203 (2016)\\n\\n22. Klaiman, S., Lehne, M.: Docreader: bounding-box free training of a document information extraction model. In: Document Analysis and Recognition–ICDAR 2021:\\n16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I 16. pp. 451–465. Springer (2021)\\n\\n23. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer,\\nL., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., Zhai, X.:\\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\\nInternational Conference on Learning Representations (2021)\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fd083ee-8201-4836-a312-c32bf10d60df', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 18, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='18 M. Dhouib et al.\\n\\n24. Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., Heard, J.: Building\\na test collection for complex document information processing. In: Proceedings of\\nthe 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 665–666. SIGIR ’06, Association for Computing\\nMachinery, New York, NY, USA (2006)\\n\\n25. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for\\nnatural language generation, translation, and comprehension. In: Proceedings of\\nthe 58th Annual Meeting of the Association for Computational Linguistics. pp.\\n7871–7880. Association for Computational Linguistics, Online (Jul 2020)\\n\\n26. Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with\\ndifferentiable binarization and adaptive scale fusion. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence 45(1), 919–931 (2022)\\n\\n27. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\\npyramid networks for object detection. In: Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition. pp. 2117–2125 (2017)\\n\\n28. Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph convolution for multimodal information extraction from visually rich documents. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers). pp. 32–39.\\nAssociation for Computational Linguistics, Minneapolis, Minnesota (Jun 2019)\\n\\n29. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692 (2019)\\n\\n30. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\\nof the IEEE/CVF international conference on computer vision. pp. 10012–10022\\n(2021)\\n\\n31. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for\\nthe 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. pp. 11976–11986 (2022)\\n\\n32. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. in\\niclr, 2017. arXiv preprint arXiv:1608.03983 (2016)\\n\\n33. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\\nConference on Learning Representations (2017)\\n\\n34. Medvet, E., Bartoli, A., Davanzo, G.: A probabilistic approach to printed document\\nunderstanding. Int. J. Doc. Anal. Recognit. 14(4), 335–347 (dec 2011)\\n\\n35. Palm, R.B., Winther, O., Laws, F.: Cloudscan-a configuration-free invoice analysis\\nsystem using recurrent neural networks. In: 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). vol. 1, pp. 406–413. IEEE\\n(2017)\\n\\n36. Powalski, R., Borchmann, �L., Jurkiewicz, D., Dwojak, T., Pietruszka, M., Pa�lka,\\nG.: Going full-tilt boogie on document understanding with text-image-layout transformer. In: Document Analysis and Recognition–ICDAR 2021: 16th International\\nConference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part II\\n16. pp. 732–747. Springer (2021)\\n\\n37. Rusi˜nol, M., Benkhelfallah, T., dAndecy, V.P.: Field extraction from administrative documents by incremental structural templates. In: 2013 12th International\\nConference on Document Analysis and Recognition. pp. 1100–1104 (2013)\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='821f6519-2556-40aa-b429-e8d232871ff5', embedding=None, metadata={'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230503011511Z', 'modDate': 'D:20230503011511Z', 'trapped': '', 'encryption': None, 'page': 19, 'total_pages': 19, 'file_path': 'c:\\\\Users\\\\shres\\\\Desktop\\\\DocParser.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DocParser 19\\n\\n38. Schaefer, R., Neudecker, C.: A two-step approach for automatic OCR postcorrection. In: Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature. pp. 52–57. International Committee on Computational Linguistics, Online\\n(Dec 2020)\\n\\n39. Sun, H., Kuang, Z., Yue, X., Lin, C., Zhang, W.: Spatial dual-modality graph\\nreasoning for key information extraction. arXiv preprint arXiv:2103.14470 (2021)\\n\\n40. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\\nL., Polosukhin, I.: Attention is all you need. Advances in neural information pro-�\\ncessing systems 30 (2017)\\n\\n41. Wang, J., Liu, C., Jin, L., Tang, G., Zhang, J., Zhang, S., Wang, Q., Wu, Y., Cai,\\nM.: Towards robust visual information extraction in real world: New dataset and\\nnovel solution. In: Proceedings of the AAAI Conference on Artificial Intelligence.\\nvol. 35, pp. 2738–2745 (2021)\\n\\n42. Wang, W., Yang, N., Wei, F., Chang, B., Zhou, M.: Gated self-matching networks for reading comprehension and question answering. In: Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers). pp. 189–198. Association for Computational Linguistics, Vancouver,\\nCanada (Jul 2017)\\n\\n43. Wei, M., He, Y., Zhang, Q.: Robust layout-aware ie for visually rich documents\\nwith pre-trained language models. In: Proceedings of the 43rd International ACM\\nSIGIR Conference on Research and Development in Information Retrieval. pp.\\n2367–2376 (2020)\\n\\n44. Xie, S., Girshick, R., Doll´ar, P., Tu, Z., He, K.: Aggregated residual transformations\\nfor deep neural networks. In: Proceedings of the IEEE conference on computer\\nvision and pattern recognition. pp. 1492–1500 (2017)\\n\\n45. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang,\\nC., Che, W., Zhang, M., Zhou, L.: LayoutLMv2: Multi-modal pre-training for\\nvisually-rich document understanding. In: Proceedings of the 59th Annual Meeting\\nof the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). pp. 2579–\\n2591. Association for Computational Linguistics, Online (Aug 2021)\\n\\n46. Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: LayoutLM: Pre-training of\\ntext and layout for document image understanding. In: Proceedings of the 26th\\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM (aug 2020)\\n\\n47. Zhao, X., Niu, E., Wu, Z., Wang, X.: Cutie: Learning to understand documents with convolutional universal text information extractor. arXiv preprint\\narXiv:1903.12363 (2019)\\n\\n\\n-----\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()  # Replace with actual implementation\n",
    "documents = llama_reader.load_data(r'c:\\Users\\shres\\Desktop\\DocParser.pdf')\n",
    "chat_service = injector.get(IndexManager)\n",
    "chat_service.ingest(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 nodes\n",
      "Node 1:\n",
      "  Content: 2 M. Dhouib et al.\n",
      "\n",
      "of all, these approaches need positional annotations along with textual annotati...\n",
      "  Score: 0.754372\n",
      "Node 2:\n",
      "  Content: DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents\n",
      "\n",
      "Mohamed Dhouib[⋆...\n",
      "  Score: 0.6434012\n",
      "Node 3:\n",
      "  Content: 6 Conclusion\n",
      "\n",
      "We have introduced DocParser, a fast end-to-end approach for information extraction fr...\n",
      "  Score: 0.6013237\n",
      "Node 4:\n",
      "  Content: DocParser 13\n",
      "\n",
      "**Table 5. The effect of decreasing the width of the feature map in various**\n",
      "**stages...\n",
      "  Score: 0.5684308\n",
      "Node 5:\n",
      "  Content: 16 M. Dhouib et al.\n",
      "\n",
      "to-end models, DocParser’s encoder is specifically designed to capture both\n",
      "int...\n",
      "  Score: 0.5668872\n",
      "Error occurred: timed out\n"
     ]
    }
   ],
   "source": [
    "chat_service = injector.get(ChatService)\n",
    "inject_service = injector.get(IndexManager)\n",
    "message = \"What is docparser?\"\n",
    "response = chat_service.chat(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but an error occurred while processing your request. Please try again or contact support if the issue persists.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
